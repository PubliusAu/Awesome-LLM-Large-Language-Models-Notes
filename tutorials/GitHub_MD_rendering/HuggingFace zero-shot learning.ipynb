{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What?** HuggingFace zero-shot learning\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jl886b1gz61_"
   },
   "source": [
    "# Zero, one and few-shot learning\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "- **What is the problem?** Traditional ML models can’t discriminate classes that are not present in training datasets, whereas few-shot learning techniques enable ML models to separate two classes that are not present in the training data and in some applications they can even separate more than two unseen classes.\n",
    "- **Few-shot** learning aims for ML models to predict the correct class of instances when a small number of examples are available in the training dataset. \n",
    "- **Zero-shot** learning aims to predict the correct class without being exposed to any instances belonging to that class in the training dataset. It is called like this because you don’t need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want!\n",
    "- **One-shot** learning is a special type of few-shot learning problem, where the aim is to learn information about object categories from one training sample/image. Face-recognition technology used by smartphones is an example of a one-shot learning problem.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T08:40:42.404224Z",
     "start_time": "2023-01-15T08:40:42.400544Z"
    },
    "id": "OWQHIeq9z62i"
   },
   "source": [
    "# Zero-shot classification\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- What do we do when we have to classify texts that haven’t been labelled. One solution would be to annotate new data and this requires expertise and and it would be time consuming.\n",
    "- Xero-shot-classification offers a solution: it allows you to specify which labels to use for the classification, so you don’t have to rely on the labels of the pretrained model. \n",
    "- **What is interesting** is the fact that you chose the labels and these do no have to be present in the original pretrained model!\n",
    "    \n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCaog4PHz62n"
   },
   "source": [
    "# Imports\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T08:45:00.176351Z",
     "start_time": "2023-01-15T08:44:51.571462Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using a pre-trained model\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T08:50:01.521572Z",
     "start_time": "2023-01-15T08:49:55.304053Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T08:51:23.044087Z",
     "start_time": "2023-01-15T08:51:23.039143Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_args_parser',\n",
       " '_batch_size',\n",
       " '_ensure_tensor_on_device',\n",
       " '_forward',\n",
       " '_forward_params',\n",
       " '_num_workers',\n",
       " '_parse_and_tokenize',\n",
       " '_postprocess_params',\n",
       " '_preprocess_params',\n",
       " '_sanitize_parameters',\n",
       " 'binary_output',\n",
       " 'call_count',\n",
       " 'check_model_type',\n",
       " 'default_input_names',\n",
       " 'device',\n",
       " 'device_placement',\n",
       " 'ensure_tensor_on_device',\n",
       " 'entailment_id',\n",
       " 'feature_extractor',\n",
       " 'forward',\n",
       " 'framework',\n",
       " 'get_inference_context',\n",
       " 'get_iterator',\n",
       " 'iterate',\n",
       " 'model',\n",
       " 'modelcard',\n",
       " 'postprocess',\n",
       " 'predict',\n",
       " 'preprocess',\n",
       " 'run_multi',\n",
       " 'run_single',\n",
       " 'save_pretrained',\n",
       " 'task',\n",
       " 'tokenizer',\n",
       " 'transform']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T08:55:13.303631Z",
     "start_time": "2023-01-15T08:55:13.299792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ZeroShotClassificationPipeline.__init__ of <transformers.pipelines.zero_shot_classification.ZeroShotClassificationPipeline object at 0x7f843ebb6760>>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T08:55:17.764284Z",
     "start_time": "2023-01-15T08:55:17.754921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_args_parser': <transformers.pipelines.zero_shot_classification.ZeroShotClassificationArgumentHandler at 0x7f843eb7f6d0>,\n",
       " 'task': 'zero-shot-classification',\n",
       " 'model': BartForSequenceClassification(\n",
       "   (model): BartModel(\n",
       "     (shared): Embedding(50265, 1024, padding_idx=1)\n",
       "     (encoder): BartEncoder(\n",
       "       (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "       (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "       (layers): ModuleList(\n",
       "         (0): BartEncoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (activation_fn): GELUActivation()\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (1): BartEncoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (activation_fn): GELUActivation()\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (2): BartEncoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (activation_fn): GELUActivation()\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (3): BartEncoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (activation_fn): GELUActivation()\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (4): BartEncoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (activation_fn): GELUActivation()\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (5): BartEncoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (activation_fn): GELUActivation()\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (6): BartEncoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (activation_fn): GELUActivation()\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (7): BartEncoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (activation_fn): GELUActivation()\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (8): BartEncoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (activation_fn): GELUActivation()\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (9): BartEncoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (activation_fn): GELUActivation()\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (10): BartEncoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (activation_fn): GELUActivation()\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (11): BartEncoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (activation_fn): GELUActivation()\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "       (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "     (decoder): BartDecoder(\n",
       "       (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "       (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "       (layers): ModuleList(\n",
       "         (0): BartDecoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (activation_fn): GELUActivation()\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (encoder_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (1): BartDecoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (activation_fn): GELUActivation()\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (encoder_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (2): BartDecoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (activation_fn): GELUActivation()\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (encoder_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (3): BartDecoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (activation_fn): GELUActivation()\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (encoder_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (4): BartDecoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (activation_fn): GELUActivation()\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (encoder_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (5): BartDecoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (activation_fn): GELUActivation()\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (encoder_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (6): BartDecoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (activation_fn): GELUActivation()\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (encoder_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (7): BartDecoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (activation_fn): GELUActivation()\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (encoder_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (8): BartDecoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (activation_fn): GELUActivation()\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (encoder_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (9): BartDecoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (activation_fn): GELUActivation()\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (encoder_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (10): BartDecoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (activation_fn): GELUActivation()\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (encoder_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "         (11): BartDecoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (activation_fn): GELUActivation()\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (encoder_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "       (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "   )\n",
       "   (classification_head): BartClassificationHead(\n",
       "     (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "     (dropout): Dropout(p=0.0, inplace=False)\n",
       "     (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'tokenizer': PreTrainedTokenizerFast(name_or_path='facebook/bart-large-mnli', vocab_size=50265, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}),\n",
       " 'feature_extractor': None,\n",
       " 'modelcard': None,\n",
       " 'framework': 'pt',\n",
       " 'device': device(type='cpu'),\n",
       " 'binary_output': False,\n",
       " 'call_count': 2,\n",
       " '_batch_size': None,\n",
       " '_num_workers': None,\n",
       " '_preprocess_params': {},\n",
       " '_forward_params': {},\n",
       " '_postprocess_params': {}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T08:50:02.534877Z",
     "start_time": "2023-01-15T08:50:01.529848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445990085601807, 0.11197411268949509, 0.04342690110206604]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T08:50:18.998017Z",
     "start_time": "2023-01-15T08:50:17.712459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'physics', 'politics'],\n",
       " 'scores': [0.7747078537940979,\n",
       "  0.10270818322896957,\n",
       "  0.08275066316127777,\n",
       "  0.039833296090364456]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\", \"physics\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKxlgIamz62s"
   },
   "source": [
    "# References\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-14T19:58:06.067788Z",
     "start_time": "2023-01-14T19:58:06.067767Z"
    },
    "id": "OETViAeCz62s"
   },
   "source": [
    "\n",
    "- https://huggingface.co/course/chapter1/3?fw=pt\n",
    "- https://research.aimultiple.com/few-shot-learning/ \n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-15T08:48:56.321020Z",
     "start_time": "2023-01-15T08:48:56.266365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Python implementation: CPython\n",
      "Python version       : 3.9.7\n",
      "IPython version      : 7.29.0\n",
      "\n",
      "Compiler    : Clang 10.0.0 \n",
      "OS          : Darwin\n",
      "Release     : 21.4.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 12\n",
      "Architecture: 64bit\n",
      "\n",
      "json    : 2.0.9\n",
      "numpy   : 1.22.2\n",
      "autopep8: 1.6.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -iv -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "298.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
