# Awesome-LLM-Large-Language-Models-Notes
***

## Known LLM models
- ALBERT
- BART | BERT | Big Bird | BLOOM |
- Chinchilla | CLIP | CTRL |
- DALL-E | DALL-E-2 | Decision Transformers | DialoGPT | DistilBERT | DQ-BART |
- ELECTRA | ERNIE |
- Flamingo |
- Gato | Gopher |GLaM | GLIDE | GC-ViT | GPT | GPT-2 | GPT-3 | GPT-Neo | GPTInstruct |
- Imagen
- Jurassic-1
- LAMDA
- mBART | Megatron | Minerva | MT-NLG
- OPT
- Palm | Pegasus
- RoBERTa
 -SeeKer | Swin Transformer | Switch
- T5 | Trajectory Transformers | Transformer XL | Turing-NLG
- ViT
- Wu Dao 2.0 |
- XLM-RoBERTa | XLNet
***

## What so special about HuggingFace?
- HuggingFace, a popular NLP library, but it also offers an easy way to deploy models via their Inference API. When you build a model using the HuggingFace library, you can then train it and upload it to their Model Hub. From there, they offer a scalable compute backend which serves models hosted in the hub. With just a few lines of code, and for the price of a few dollars per day, anyone can deploy secure, scalable NLP models built with the HuggingFace library. Read more about this [here](https://huggingface.co/pricing).
***

## Must-Read Papers on LLM
- https://github.com/thunlp/PLMpapers
*** 

## Blogs
- [Building a synth with ChatGPT](https://jlongster.com/building-a-synth-with-chatgpt)
- [PubMed GPT: a Domain-Specific Large Language Model for Biomedical Text](https://www.mosaicml.com/blog/introducing-pubmed-gpt)
- [ChatGPT - Where it lacks](https://cookup.ai/chatgpt/where-it-lacks/)
- [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts)
- [ChatGPT vs. GPT3: The Ultimate Comparison](https://dzone.com/articles/chatgpt-vs-gpt3-the-ultimate-comparison-features)
- [Prompt Engineering 101: Introduction and resources](https://amatriain.net/blog/PromptEngineering)
- [Transformer models: an introduction and catalog — 2022 Edition](https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/#GATO)
- [Can GPT-3 or BERT Ever Understand Language?⁠—The Limits of Deep Learning Language Models](https://neptune.ai/blog/gpt-3-bert-limits-of-deep-learning-language-models)
- [10 Things You Need to Know About BERT and the Transformer Architecture That Are Reshaping the AI Landscape](https://neptune.ai/blog/bert-and-the-transformer-architecture)
- [Comprehensive Guide to Transformers](https://neptune.ai/blog/comprehensive-guide-to-transformers)
- [Unmasking BERT: The Key to Transformer Model Performance](https://neptune.ai/blog/unmasking-bert-transformer-model-performance)
- [Transformer NLP Models (Meena and LaMDA): Are They “Sentient” and What Does It Mean for Open-Domain Chatbots?](https://neptune.ai/blog/transformer-nlp-models-meena-lamda-chatbots)
- [Hugging Face Pre-trained Models: Find the Best One for Your Task](https://neptune.ai/blog/hugging-face-pre-trained-models-find-the-best)
***

## Start-up funding landscape
- [NLP Startup Funding in 2022](https://towardsdatascience.com/nlp-startup-funding-in-2022-caad77cb0f0)
***

## Available tutorials
- [Building a search engine with a pre-trained BERT model]()
- [Fine tuning pre-trained BERT model on Text Classification Task]()
- [Fine tuning pre-trained BERT model on the Amazon product review dataset]()
- [Sentiment analysis with Hugging Face transformer]()
***

## How to in Google Colab
- The easiest option would be for you to clone this repository.
- Navigate to Google Colab and open the notebook directly from Colab.
- You can then also write it back to GitHub provided permission to Colab. The whole procedure is automated.
***

## Implementations from scratch
- [How to Code BERT Using PyTorch](https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial)
- [miniGPT in PyTorch](https://github.com/karpathy/minGPT)
- [nanoGPT in PyTorch](https://github.com/karpathy/nanoGPT)
***
